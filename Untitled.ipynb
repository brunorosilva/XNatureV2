{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XNature V2 Classifier\n",
    "\n",
    "\n",
    "The XNature V2 dataset contains x-ray images of 8 diffent classes:\n",
    "<ol>\n",
    "<li>Fruits\n",
    "<li>Guns\n",
    "<li>Keys\n",
    "<li>Knifes\n",
    "<li>Razors\n",
    "<li>Salmons\n",
    "<li>Shurikens\n",
    "<li>Wood\n",
    "</ol>\n",
    "\n",
    "The objectives of this notebook are explore the dataset, preprocess the image data and to create a image classifier using Deep Learning.<br><br>\n",
    "For this particular task it'll be used the Keras API, since it's faster and simpler than using TensorFlow to test lots of hypotesis using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "mypath = \"C:\\\\Users\\\\rodri\\\\Desktop\\\\xnaturev2\\\\XNature\"\n",
    "pastas = listdir(mypath)\n",
    "pastas.remove('.ipynb_checkpoints')\n",
    "pastas.remove('Untitled.ipynb')\n",
    "pastas.remove('Untitled1.ipynb')\n",
    "pastas.remove('XNature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "x = []\n",
    "y = []\n",
    "siz = (30,30)\n",
    "\n",
    "for pasta in pastas:\n",
    "    path_pastas = mypath + '\\\\' + pasta\n",
    "    files = listdir(path_pastas)\n",
    "    for file in files:\n",
    "        img = Image.open(path_pastas + '\\\\'+ file)\n",
    "        img_rsz = img.resize(siz)\n",
    "        x.append(np.asarray(img_rsz))\n",
    "        y.append(pasta)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at these images\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "rand = np.random.randint(0, x.shape[0], 20)\n",
    "sample_imgs = x[rand]\n",
    "sample_labels = y[rand]\n",
    "\n",
    "# code to view the images\n",
    "\n",
    "num_rows, num_cols = 2, 10\n",
    "f, ax = plt.subplots(num_rows, num_cols, figsize=(12,5),\n",
    "                     gridspec_kw={'wspace':0.03, 'hspace':0.01}, \n",
    "                     squeeze=True)\n",
    "\n",
    "for r in range(num_rows):\n",
    "    for c in range(num_cols):\n",
    "        image_index = r * num_cols + c\n",
    "        ax[r,c].axis(\"off\")\n",
    "        ax[r,c].imshow(sample_imgs[image_index], cmap='gray')\n",
    "        ax[r,c].set_title(sample_labels[image_index])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('So fat we\\'ve seen how some of these images look like in grayscale and we are able to address each of the '+str(x.shape[0])+' images\\ninto '+str(len(np.unique(y)))+' unique categories, just as it was described earlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating the data into train and test\n",
    "\n",
    "For this particular part it'll be used the sklearn's train test split, because it's a very simple way to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# defining test size and fixed random state for reproducibility purposes\n",
    "test_size = .20\n",
    "random_state = 42\n",
    "\n",
    "# transforming y labels into numbers\n",
    "LE = LabelEncoder()\n",
    "y_lab = LE.fit_transform(y)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_lab, test_size = test_size, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\")\n",
    "print(\"Number of train imgs:\", x_train.shape[0])\n",
    "print(\"Number of test imgs:\", x_test.shape[0])\n",
    "print(\"Number of train labels:\", len(np.unique(y_train)))\n",
    "print(\"Number of test imgs:\", len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribuition of labels in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements_train, counts_elements_train = np.unique(y_train, return_counts=True)\n",
    "unique_elements_test, counts_elements_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print('In the training dataset: ')\n",
    "\n",
    "for i in range(len(unique_elements_train)):\n",
    "    print('Class {0} {1:.2f}%'.\n",
    "          format(unique_elements_train[i], counts_elements_train[i]/len(y_train)*100))\n",
    "\n",
    "print('\\n\\nIn the testing dataset: ')\n",
    "for i in range(len(unique_elements_test)):\n",
    "\n",
    "    print('Class {0} {1:.2f}%'.\n",
    "          format(unique_elements_test[i], counts_elements_test[i]/len(y_test)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asserting the data above, it can be issued that both the train and the test arrays have a close proximity in distribuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "img_height = sample_imgs[0].shape[0]\n",
    "img_width = sample_imgs[0].shape[1]\n",
    "num_channels = 1\n",
    "\n",
    "train_data = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2], num_channels))\n",
    "test_data = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2], num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32') / 255.\n",
    "test_data = test_data.astype('float32') / 255.\n",
    "\n",
    "num_classes = 8\n",
    "train_labels_cat = to_categorical(y_train,num_classes)\n",
    "test_labels_cat = to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5): \n",
    "    indexes = np.random.permutation(len(train_data))\n",
    "\n",
    "train_data = train_data[indexes]\n",
    "train_labels_cat = train_labels_cat[indexes]\n",
    "\n",
    "# now set-aside 10% of the train_data/labels as the\n",
    "# cross-validation sets\n",
    "val_perc = 0.10\n",
    "val_count = int(val_perc * len(train_data))\n",
    "\n",
    "# first pick validation set from train_data/labels\n",
    "val_data = train_data[:val_count,:]\n",
    "val_labels_cat = train_labels_cat[:val_count,:]\n",
    "train_data2 = train_data[val_count:,:]\n",
    "train_labels_cat2 = train_labels_cat[val_count:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2,2), padding = 'same', activation = 'relu',  input_shape=(img_height, img_width, num_channels)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(64, (2,2), padding = 'same', activation = 'relu',  input_shape=(img_height, img_width, num_channels)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(128, (2,2), padding = 'same', activation = 'relu',  input_shape=(img_height, img_width, num_channels)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(8, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(train_data2, train_labels_cat2, \n",
    "                    epochs=15, batch_size=64,\n",
    "                    validation_data=(val_data, val_labels_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(history, plot_title=None, fig_size=None):\n",
    "    \"\"\" Useful function to view plot of loss values & accuracies across the various epochs\n",
    "        Works with the history object returned by the train_model(...) call \"\"\"\n",
    "    assert type(history) is dict\n",
    "\n",
    "    # NOTE: the history object should always have loss & acc (for training data), but MAY have\n",
    "    # val_loss & val_acc for validation data\n",
    "    loss_vals = history['loss']\n",
    "    val_loss_vals = history['val_loss'] if 'val_loss' in history.keys() else None\n",
    "    epochs = range(1, len(history['acc']) + 1)\n",
    "\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=((16, 4) if fig_size is None else fig_size))\n",
    "\n",
    "    # plot losses on ax[0]\n",
    "    ax[0].plot(epochs, loss_vals, color='navy', marker='o', linestyle=' ', label='Training Loss')\n",
    "    if val_loss_vals is not None:\n",
    "        ax[0].plot(epochs, val_loss_vals, color='firebrick', marker='*', label='Validation Loss')\n",
    "        ax[0].set_title('Training & Validation Loss')\n",
    "        ax[0].legend(loc='best')\n",
    "    else:\n",
    "        ax[0].set_title('Training Loss')\n",
    "\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # plot accuracies\n",
    "    acc_vals = history['acc']\n",
    "    val_acc_vals = history['val_acc'] if 'val_acc' in history.keys() else None\n",
    "\n",
    "    ax[1].plot(epochs, acc_vals, color='navy', marker='o', ls=' ', label='Training Accuracy')\n",
    "    if val_acc_vals is not None:\n",
    "        ax[1].plot(epochs, val_acc_vals, color='firebrick', marker='*', label='Validation Accuracy')\n",
    "        ax[1].set_title('Training & Validation Accuracy')\n",
    "        ax[1].legend(loc='best')\n",
    "    else:\n",
    "        ax[1].set_title('Training Accuracy')\n",
    "\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    if plot_title is not None:\n",
    "        plt.suptitle(plot_title)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # delete locals from heap before exiting (to save some memory!)\n",
    "    del loss_vals, epochs, acc_vals\n",
    "    if val_loss_vals is not None:\n",
    "        del val_loss_vals\n",
    "    if val_acc_vals is not None:\n",
    "        del val_acc_vals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(results.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now into the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
